{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6SvVG8-AkAW",
        "outputId": "94d694ff-cd07-4512-90ba-c887116a4a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "greet_inputs = ('hello', 'hi', 'whassup', 'how are you?')\n",
        "greet_responses = ('hi', 'Hey', 'Hey There!', 'There there!!')\n",
        "\n",
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greet_inputs:\n",
        "            return random.choice(greet_responses)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
        "\n",
        "\n",
        "with open('data.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "sentence_token = nltk.sent_tokenize(data)\n",
        "word_tokens = []\n",
        "\n",
        "def response(user_response):\n",
        "    robo1_response = ''\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sentence_token)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "\n",
        "    if len(vals[0]) > 1:\n",
        "        idx = vals.argsort()[0][-2]\n",
        "        flat = vals.flatten()\n",
        "        flat.sort()\n",
        "        req_tfidf = flat[-2]\n",
        "\n",
        "        if req_tfidf == 0:\n",
        "            robo1_response = \"I am sorry, not able to understand your query/question\"\n",
        "        else:\n",
        "            robo1_response = sentence_token[idx]\n",
        "    else:\n",
        "        robo1_response = \"I am sorry, not able to understand your query/question\"\n",
        "\n",
        "    return robo1_response\n",
        "\n",
        "flag = True\n",
        "\n",
        "print('Hello! I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending the convo type bye!')\n",
        "while flag:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thank you' or user_response == 'thanks':\n",
        "            flag = False\n",
        "            print('Bot: You are Welcome..')\n",
        "        else:\n",
        "            if greet(user_response) is not None:\n",
        "                print('Bot:', greet(user_response))\n",
        "            else:\n",
        "                sentence_token.append(user_response)\n",
        "                word_tokens += nltk.word_tokenize(user_response)\n",
        "                final_words = list(set(word_tokens))\n",
        "                print(\"Bot:\", response(user_response))\n",
        "                sentence_token.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"Bot: Good Bye!!\")"
      ],
      "metadata": {
        "id": "OVPPkiWHG9qI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9a5ca7-1e44-471d-e7b3-57ab4be6fb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending the convo type bye!\n"
          ]
        }
      ]
    }
  ]
}